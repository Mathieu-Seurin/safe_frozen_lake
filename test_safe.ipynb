{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lake import FrozenLakeEnvStop\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': 0, 'gave_feedback': False}\n\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\nNone\n({'state': 0, 'gave_feedback': False}, 0.0, False, {'prob': 1.0})\n({'state': 4, 'gave_feedback': False}, 0.0, False, {'prob': 1.0})\n({'state': 8, 'gave_feedback': False}, 0.0, False, {'prob': 1.0})\n({'state': 8, 'gave_feedback': True}, 0, False, {'prob': 1.0})\n"
     ]
    }
   ],
   "source": [
    "print(game.reset())\n",
    "print(game.render())\n",
    "print(game.step(0))\n",
    "print(game.step(1))\n",
    "print(game.step(1))\n",
    "print(game.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\nDiscrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(game.observation_space)\n",
    "print(game.action_space)\n",
    "# action are : 0 Left     1 Down      2  RIGHT     3 UP    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_expe(create_algo, game, max_ep, verbose=True):\n",
    "    \n",
    "    count_success = 0\n",
    "    success_to_win = 5\n",
    "    \n",
    "    algo = create_algo()\n",
    "    \n",
    "    total_steps = 0\n",
    "    total_feedback_list = []\n",
    "    \n",
    "    for ep in range(max_ep):\n",
    "        \n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Init information to save per episode\n",
    "        cumul_reward = 0\n",
    "        num_steps_this_ep = 0\n",
    "        num_feedback_this_ep = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            action = algo.choose_action_eps_greedy(state)\n",
    "            next_state, reward, done, info = game.step(action)\n",
    "            algo.optimize(state, action, next_state, reward)\n",
    "            \n",
    "            # Saving some info from env\n",
    "            num_feedback_this_ep += next_state['gave_feedback']\n",
    "            cumul_reward += reward\n",
    "            state = next_state\n",
    "            num_steps_this_ep += 1\n",
    "        \n",
    "        #if last reward is 1, we consider that the environment is solved.\n",
    "        # if the algo solve the algo N times in a row, '''with the best policy''' => it's okay ! (N = success_to_win)\n",
    "        if reward == 1 and num_steps_this_ep <= game.shortest_path_length :\n",
    "            count_success += 1\n",
    "        else:\n",
    "            count_success = 0\n",
    "        \n",
    "        total_feedback_list.append(num_feedback_this_ep)\n",
    "        total_steps += num_steps_this_ep\n",
    "            \n",
    "        if count_success > success_to_win:\n",
    "            break\n",
    "        if verbose:\n",
    "            print(\"End of ep #{:05d} in {:03d} steps, cumul_reward = {}\".format(ep,  num_steps_this_ep, cumul_reward))\n",
    "    \n",
    "    run_info = dict()\n",
    "    run_info['last_ep_number'] = ep\n",
    "    run_info['last_ep_timestep'] = num_steps_this_ep\n",
    "    run_info['total_steps'] = total_steps\n",
    "    run_info['feedback_number_list'] = total_feedback_list\n",
    "    \n",
    "    return run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_expe(create_algo, game, n_expe):\n",
    "    \n",
    "    max_ep = 1000\n",
    "    \n",
    "    mean_num_ep = np.zeros(n_expe)\n",
    "    num_feedback_per_ep = np.zeros((n_expe, max_ep))\n",
    "    total_steps_mean = np.zeros(n_expe)\n",
    "    \n",
    "    for i in range(n_expe):\n",
    "        run_info = run_expe(create_algo, game, max_ep=max_ep, verbose=False)\n",
    "        mean_num_ep[i] = run_info['last_ep_number']\n",
    "        \n",
    "        assert run_info['last_ep_timestep'] == game.shortest_path_length or mean_num_ep[i] == max_ep-1, \"Problem, expes terminated prematurely\"\n",
    "        \n",
    "        num_feedback_list = run_info['feedback_number_list']\n",
    "        num_feedback_per_ep[i, :len(num_feedback_list) ] = num_feedback_list\n",
    "        \n",
    "        total_steps_mean[i] = run_info[\"total_steps\"]\n",
    "\n",
    "    print(\"Number of episodes mean {}, std {}\".format(np.mean(mean_num_ep), np.std(mean_num_ep)))\n",
    "    print(\"Number of failure to solve env\", np.count_nonzero(mean_num_ep==max_ep - 1))\n",
    "    \n",
    "    episodes_of_non_failure = mean_num_ep[np.where(mean_num_ep<max_ep-1)]\n",
    "    print(\"Number of episodes when not failing\", episodes_of_non_failure.mean())\n",
    "    \n",
    "    print(\"Total steps to solve environment :\", )\n",
    "    \n",
    "    sns.tsplot(data=num_feedback_per_ep)\n",
    "    sns.tsplot(data=np.zeros(n_expe))\n",
    "    plt.title(\"Number of time feedback is being used\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabQLearning(object):\n",
    "    def __init__(self, env_size, n_action, gamma, lr, expected_exploration_steps):\n",
    "        \n",
    "        self.n_action = n_action\n",
    "        self.env_size = env_size\n",
    "        self.q_tab = np.zeros((env_size, n_action))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.expected_exploration_steps = expected_exploration_steps\n",
    "        self.n_step_eps = 0\n",
    "        self.minimum_epsilon = 0.05\n",
    "        self.epsilon_init = 1   \n",
    "        self.current_eps = self.epsilon_init\n",
    "        \n",
    "    def choose_action_eps_greedy(self, s):\n",
    "        \n",
    "        if np.random.random() < self.current_eps:\n",
    "            a = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            s = s['state']\n",
    "            a = np.argmax(self.q_tab[s])\n",
    "            \n",
    "        self.current_eps = max(self.minimum_epsilon, self.epsilon_init * np.exp(- 2.5 * self.n_step_eps / self.expected_exploration_steps))\n",
    "        self.n_step_eps += 1\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def optimize(self, state, a, next_state, r):\n",
    "        state, next_state = state['state'], next_state['state']\n",
    "        self.q_tab[state, a] = (1-self.lr)*self.q_tab[state, a] + self.lr*(r + self.gamma*np.max(self.q_tab[next_state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #00000 in 092 steps, cumul_reward = 1.0\nEnd of ep #00001 in 100 steps, cumul_reward = 0.0\nEnd of ep #00002 in 070 steps, cumul_reward = 1.0\nEnd of ep #00003 in 019 steps, cumul_reward = 1.0\nEnd of ep #00004 in 035 steps, cumul_reward = 1.0\nEnd of ep #00005 in 028 steps, cumul_reward = 1.0\nEnd of ep #00006 in 029 steps, cumul_reward = 1.0\nEnd of ep #00007 in 024 steps, cumul_reward = 1.0\nEnd of ep #00008 in 009 steps, cumul_reward = 1.0\nEnd of ep #00009 in 014 steps, cumul_reward = 1.0\nEnd of ep #00010 in 017 steps, cumul_reward = 1.0\nEnd of ep #00011 in 020 steps, cumul_reward = 1.0\nEnd of ep #00012 in 012 steps, cumul_reward = 1.0\nEnd of ep #00013 in 015 steps, cumul_reward = 1.0\nEnd of ep #00014 in 015 steps, cumul_reward = 1.0\nEnd of ep #00015 in 023 steps, cumul_reward = 1.0\nEnd of ep #00016 in 017 steps, cumul_reward = 1.0\nEnd of ep #00017 in 012 steps, cumul_reward = 1.0\nEnd of ep #00018 in 035 steps, cumul_reward = 1.0\nEnd of ep #00019 in 024 steps, cumul_reward = 1.0\nEnd of ep #00020 in 019 steps, cumul_reward = 1.0\nEnd of ep #00021 in 006 steps, cumul_reward = 1.0\nEnd of ep #00022 in 008 steps, cumul_reward = 1.0\nEnd of ep #00023 in 015 steps, cumul_reward = 1.0\nEnd of ep #00024 in 010 steps, cumul_reward = 1.0\nEnd of ep #00025 in 012 steps, cumul_reward = 1.0\nEnd of ep #00026 in 009 steps, cumul_reward = 1.0\nEnd of ep #00027 in 009 steps, cumul_reward = 1.0\nEnd of ep #00028 in 022 steps, cumul_reward = 1.0\nEnd of ep #00029 in 009 steps, cumul_reward = 1.0\nEnd of ep #00030 in 016 steps, cumul_reward = 1.0\nEnd of ep #00031 in 016 steps, cumul_reward = 1.0\nEnd of ep #00032 in 012 steps, cumul_reward = 1.0\nEnd of ep #00033 in 009 steps, cumul_reward = 1.0\nEnd of ep #00034 in 007 steps, cumul_reward = 1.0\nEnd of ep #00035 in 014 steps, cumul_reward = 1.0\nEnd of ep #00036 in 021 steps, cumul_reward = 1.0\nEnd of ep #00037 in 010 steps, cumul_reward = 1.0\nEnd of ep #00038 in 014 steps, cumul_reward = 1.0\nEnd of ep #00039 in 017 steps, cumul_reward = 1.0\nEnd of ep #00040 in 006 steps, cumul_reward = 1.0\nEnd of ep #00041 in 012 steps, cumul_reward = 1.0\nEnd of ep #00042 in 009 steps, cumul_reward = 1.0\nEnd of ep #00043 in 007 steps, cumul_reward = 1.0\nEnd of ep #00044 in 009 steps, cumul_reward = 1.0\nEnd of ep #00045 in 009 steps, cumul_reward = 1.0\nEnd of ep #00046 in 008 steps, cumul_reward = 1.0\nEnd of ep #00047 in 011 steps, cumul_reward = 1.0\nEnd of ep #00048 in 027 steps, cumul_reward = 1.0\nEnd of ep #00049 in 014 steps, cumul_reward = 1.0\nEnd of ep #00050 in 008 steps, cumul_reward = 1.0\nEnd of ep #00051 in 006 steps, cumul_reward = 1.0\nEnd of ep #00052 in 012 steps, cumul_reward = 1.0\nEnd of ep #00053 in 008 steps, cumul_reward = 1.0\nEnd of ep #00054 in 008 steps, cumul_reward = 1.0\nEnd of ep #00055 in 010 steps, cumul_reward = 1.0\nEnd of ep #00056 in 009 steps, cumul_reward = 1.0\nEnd of ep #00057 in 013 steps, cumul_reward = 1.0\nEnd of ep #00058 in 015 steps, cumul_reward = 1.0\nEnd of ep #00059 in 008 steps, cumul_reward = 1.0\nEnd of ep #00060 in 010 steps, cumul_reward = 1.0\nEnd of ep #00061 in 011 steps, cumul_reward = 1.0\nEnd of ep #00062 in 013 steps, cumul_reward = 1.0\nEnd of ep #00063 in 007 steps, cumul_reward = 1.0\nEnd of ep #00064 in 007 steps, cumul_reward = 1.0\nEnd of ep #00065 in 008 steps, cumul_reward = 1.0\nEnd of ep #00066 in 009 steps, cumul_reward = 1.0\nEnd of ep #00067 in 008 steps, cumul_reward = 1.0\nEnd of ep #00068 in 006 steps, cumul_reward = 1.0\nEnd of ep #00069 in 006 steps, cumul_reward = 1.0\nEnd of ep #00070 in 007 steps, cumul_reward = 1.0\nEnd of ep #00071 in 011 steps, cumul_reward = 1.0\nEnd of ep #00072 in 008 steps, cumul_reward = 1.0\nEnd of ep #00073 in 009 steps, cumul_reward = 1.0\nEnd of ep #00074 in 009 steps, cumul_reward = 1.0\nEnd of ep #00075 in 012 steps, cumul_reward = 1.0\nEnd of ep #00076 in 007 steps, cumul_reward = 1.0\nEnd of ep #00077 in 008 steps, cumul_reward = 1.0\nEnd of ep #00078 in 008 steps, cumul_reward = 1.0\nEnd of ep #00079 in 008 steps, cumul_reward = 1.0\nEnd of ep #00080 in 008 steps, cumul_reward = 1.0\nEnd of ep #00081 in 007 steps, cumul_reward = 1.0\nEnd of ep #00082 in 036 steps, cumul_reward = 1.0\nEnd of ep #00083 in 011 steps, cumul_reward = 1.0\nEnd of ep #00084 in 007 steps, cumul_reward = 1.0\nEnd of ep #00085 in 006 steps, cumul_reward = 1.0\nEnd of ep #00086 in 007 steps, cumul_reward = 1.0\nEnd of ep #00087 in 015 steps, cumul_reward = 1.0\nEnd of ep #00088 in 006 steps, cumul_reward = 1.0\nEnd of ep #00089 in 008 steps, cumul_reward = 1.0\nEnd of ep #00090 in 008 steps, cumul_reward = 1.0\nEnd of ep #00091 in 009 steps, cumul_reward = 1.0\nEnd of ep #00092 in 010 steps, cumul_reward = 1.0\nEnd of ep #00093 in 007 steps, cumul_reward = 1.0\nEnd of ep #00094 in 014 steps, cumul_reward = 1.0\nEnd of ep #00095 in 011 steps, cumul_reward = 1.0\nEnd of ep #00096 in 011 steps, cumul_reward = 1.0\nEnd of ep #00097 in 012 steps, cumul_reward = 1.0\nEnd of ep #00098 in 009 steps, cumul_reward = 1.0\nEnd of ep #00099 in 012 steps, cumul_reward = 1.0\nEnd of ep #00100 in 012 steps, cumul_reward = 1.0\nEnd of ep #00101 in 006 steps, cumul_reward = 1.0\nEnd of ep #00102 in 017 steps, cumul_reward = 1.0\nEnd of ep #00103 in 008 steps, cumul_reward = 1.0\nEnd of ep #00104 in 012 steps, cumul_reward = 1.0\nEnd of ep #00105 in 007 steps, cumul_reward = 1.0\nEnd of ep #00106 in 008 steps, cumul_reward = 1.0\nEnd of ep #00107 in 007 steps, cumul_reward = 1.0\nEnd of ep #00108 in 008 steps, cumul_reward = 1.0\nEnd of ep #00109 in 008 steps, cumul_reward = 1.0\nEnd of ep #00110 in 006 steps, cumul_reward = 1.0\nEnd of ep #00111 in 007 steps, cumul_reward = 1.0\nEnd of ep #00112 in 011 steps, cumul_reward = 1.0\nEnd of ep #00113 in 006 steps, cumul_reward = 1.0\nEnd of ep #00114 in 006 steps, cumul_reward = 1.0\nEnd of ep #00115 in 008 steps, cumul_reward = 1.0\nEnd of ep #00116 in 006 steps, cumul_reward = 1.0\nEnd of ep #00117 in 008 steps, cumul_reward = 1.0\nEnd of ep #00118 in 008 steps, cumul_reward = 1.0\nEnd of ep #00119 in 006 steps, cumul_reward = 1.0\nEnd of ep #00120 in 006 steps, cumul_reward = 1.0\nEnd of ep #00121 in 010 steps, cumul_reward = 1.0\nEnd of ep #00122 in 007 steps, cumul_reward = 1.0\nEnd of ep #00123 in 007 steps, cumul_reward = 1.0\nEnd of ep #00124 in 006 steps, cumul_reward = 1.0\nEnd of ep #00125 in 009 steps, cumul_reward = 1.0\nEnd of ep #00126 in 015 steps, cumul_reward = 1.0\nEnd of ep #00127 in 013 steps, cumul_reward = 1.0\nEnd of ep #00128 in 013 steps, cumul_reward = 1.0\nEnd of ep #00129 in 006 steps, cumul_reward = 1.0\nEnd of ep #00130 in 008 steps, cumul_reward = 1.0\nEnd of ep #00131 in 007 steps, cumul_reward = 1.0\nEnd of ep #00132 in 010 steps, cumul_reward = 1.0\nEnd of ep #00133 in 008 steps, cumul_reward = 1.0\nEnd of ep #00134 in 006 steps, cumul_reward = 1.0\nEnd of ep #00135 in 006 steps, cumul_reward = 1.0\nEnd of ep #00136 in 011 steps, cumul_reward = 1.0\nEnd of ep #00137 in 006 steps, cumul_reward = 1.0\nEnd of ep #00138 in 006 steps, cumul_reward = 1.0\nEnd of ep #00139 in 009 steps, cumul_reward = 1.0\nEnd of ep #00140 in 010 steps, cumul_reward = 1.0\nEnd of ep #00141 in 006 steps, cumul_reward = 1.0\nEnd of ep #00142 in 011 steps, cumul_reward = 1.0\nEnd of ep #00143 in 011 steps, cumul_reward = 1.0\nEnd of ep #00144 in 007 steps, cumul_reward = 1.0\nEnd of ep #00145 in 008 steps, cumul_reward = 1.0\nEnd of ep #00146 in 006 steps, cumul_reward = 1.0\nEnd of ep #00147 in 006 steps, cumul_reward = 1.0\nEnd of ep #00148 in 010 steps, cumul_reward = 1.0\nEnd of ep #00149 in 006 steps, cumul_reward = 1.0\nEnd of ep #00150 in 008 steps, cumul_reward = 1.0\nEnd of ep #00151 in 009 steps, cumul_reward = 1.0\nEnd of ep #00152 in 006 steps, cumul_reward = 1.0\nEnd of ep #00153 in 008 steps, cumul_reward = 1.0\nEnd of ep #00154 in 008 steps, cumul_reward = 1.0\nEnd of ep #00155 in 007 steps, cumul_reward = 1.0\nEnd of ep #00156 in 009 steps, cumul_reward = 1.0\nEnd of ep #00157 in 006 steps, cumul_reward = 1.0\nEnd of ep #00158 in 008 steps, cumul_reward = 1.0\nEnd of ep #00159 in 015 steps, cumul_reward = 1.0\nEnd of ep #00160 in 010 steps, cumul_reward = 1.0\nEnd of ep #00161 in 010 steps, cumul_reward = 1.0\nEnd of ep #00162 in 006 steps, cumul_reward = 1.0\nEnd of ep #00163 in 006 steps, cumul_reward = 1.0\nEnd of ep #00164 in 006 steps, cumul_reward = 1.0\nEnd of ep #00165 in 011 steps, cumul_reward = 1.0\nEnd of ep #00166 in 006 steps, cumul_reward = 1.0\nEnd of ep #00167 in 008 steps, cumul_reward = 1.0\nEnd of ep #00168 in 006 steps, cumul_reward = 1.0\nEnd of ep #00169 in 007 steps, cumul_reward = 1.0\nEnd of ep #00170 in 008 steps, cumul_reward = 1.0\nEnd of ep #00171 in 006 steps, cumul_reward = 1.0\nEnd of ep #00172 in 009 steps, cumul_reward = 1.0\nEnd of ep #00173 in 011 steps, cumul_reward = 1.0\nEnd of ep #00174 in 010 steps, cumul_reward = 1.0\nEnd of ep #00175 in 006 steps, cumul_reward = 1.0\nEnd of ep #00176 in 008 steps, cumul_reward = 1.0\nEnd of ep #00177 in 007 steps, cumul_reward = 1.0\nEnd of ep #00178 in 006 steps, cumul_reward = 1.0\nEnd of ep #00179 in 006 steps, cumul_reward = 1.0\nEnd of ep #00180 in 007 steps, cumul_reward = 1.0\nEnd of ep #00181 in 007 steps, cumul_reward = 1.0\nEnd of ep #00182 in 010 steps, cumul_reward = 1.0\nEnd of ep #00183 in 008 steps, cumul_reward = 1.0\nEnd of ep #00184 in 006 steps, cumul_reward = 1.0\nEnd of ep #00185 in 006 steps, cumul_reward = 1.0\nEnd of ep #00186 in 007 steps, cumul_reward = 1.0\nEnd of ep #00187 in 008 steps, cumul_reward = 1.0\nEnd of ep #00188 in 007 steps, cumul_reward = 1.0\nEnd of ep #00189 in 006 steps, cumul_reward = 1.0\nEnd of ep #00190 in 008 steps, cumul_reward = 1.0\nEnd of ep #00191 in 007 steps, cumul_reward = 1.0\nEnd of ep #00192 in 009 steps, cumul_reward = 1.0\nEnd of ep #00193 in 006 steps, cumul_reward = 1.0\nEnd of ep #00194 in 008 steps, cumul_reward = 1.0\nEnd of ep #00195 in 006 steps, cumul_reward = 1.0\nEnd of ep #00196 in 006 steps, cumul_reward = 1.0\nEnd of ep #00197 in 006 steps, cumul_reward = 1.0\nEnd of ep #00198 in 009 steps, cumul_reward = 1.0\nEnd of ep #00199 in 006 steps, cumul_reward = 1.0\nEnd of ep #00200 in 008 steps, cumul_reward = 1.0\nEnd of ep #00201 in 008 steps, cumul_reward = 1.0\nEnd of ep #00202 in 007 steps, cumul_reward = 1.0\nEnd of ep #00203 in 006 steps, cumul_reward = 1.0\nEnd of ep #00204 in 008 steps, cumul_reward = 1.0\nEnd of ep #00205 in 007 steps, cumul_reward = 1.0\nEnd of ep #00206 in 006 steps, cumul_reward = 1.0\nEnd of ep #00207 in 006 steps, cumul_reward = 1.0\nEnd of ep #00208 in 006 steps, cumul_reward = 1.0\nEnd of ep #00209 in 006 steps, cumul_reward = 1.0\nEnd of ep #00210 in 008 steps, cumul_reward = 1.0\nEnd of ep #00211 in 006 steps, cumul_reward = 1.0\nEnd of ep #00212 in 006 steps, cumul_reward = 1.0\nEnd of ep #00213 in 006 steps, cumul_reward = 1.0\nEnd of ep #00214 in 009 steps, cumul_reward = 1.0\nEnd of ep #00215 in 006 steps, cumul_reward = 1.0\nEnd of ep #00216 in 006 steps, cumul_reward = 1.0\nEnd of ep #00217 in 007 steps, cumul_reward = 1.0\nEnd of ep #00218 in 010 steps, cumul_reward = 1.0\nEnd of ep #00219 in 006 steps, cumul_reward = 1.0\nEnd of ep #00220 in 009 steps, cumul_reward = 1.0\nEnd of ep #00221 in 008 steps, cumul_reward = 1.0\nEnd of ep #00222 in 006 steps, cumul_reward = 1.0\nEnd of ep #00223 in 006 steps, cumul_reward = 1.0\nEnd of ep #00224 in 009 steps, cumul_reward = 1.0\nEnd of ep #00225 in 008 steps, cumul_reward = 1.0\nEnd of ep #00226 in 006 steps, cumul_reward = 1.0\nEnd of ep #00227 in 006 steps, cumul_reward = 1.0\nEnd of ep #00228 in 008 steps, cumul_reward = 1.0\nEnd of ep #00229 in 006 steps, cumul_reward = 1.0\nEnd of ep #00230 in 014 steps, cumul_reward = 1.0\nEnd of ep #00231 in 006 steps, cumul_reward = 1.0\nEnd of ep #00232 in 006 steps, cumul_reward = 1.0\nEnd of ep #00233 in 008 steps, cumul_reward = 1.0\nEnd of ep #00234 in 006 steps, cumul_reward = 1.0\nEnd of ep #00235 in 007 steps, cumul_reward = 1.0\nEnd of ep #00236 in 006 steps, cumul_reward = 1.0\nEnd of ep #00237 in 010 steps, cumul_reward = 1.0\nEnd of ep #00238 in 006 steps, cumul_reward = 1.0\nEnd of ep #00239 in 006 steps, cumul_reward = 1.0\nEnd of ep #00240 in 006 steps, cumul_reward = 1.0\nEnd of ep #00241 in 007 steps, cumul_reward = 1.0\nEnd of ep #00242 in 006 steps, cumul_reward = 1.0\nEnd of ep #00243 in 006 steps, cumul_reward = 1.0\nEnd of ep #00244 in 006 steps, cumul_reward = 1.0\nEnd of ep #00245 in 006 steps, cumul_reward = 1.0\nEnd of ep #00246 in 008 steps, cumul_reward = 1.0\nEnd of ep #00247 in 006 steps, cumul_reward = 1.0\nEnd of ep #00248 in 006 steps, cumul_reward = 1.0\nEnd of ep #00249 in 008 steps, cumul_reward = 1.0\nEnd of ep #00250 in 006 steps, cumul_reward = 1.0\nEnd of ep #00251 in 006 steps, cumul_reward = 1.0\nEnd of ep #00252 in 008 steps, cumul_reward = 1.0\nEnd of ep #00253 in 007 steps, cumul_reward = 1.0\nEnd of ep #00254 in 006 steps, cumul_reward = 1.0\nEnd of ep #00255 in 008 steps, cumul_reward = 1.0\nEnd of ep #00256 in 007 steps, cumul_reward = 1.0\nEnd of ep #00257 in 006 steps, cumul_reward = 1.0\nEnd of ep #00258 in 006 steps, cumul_reward = 1.0\nEnd of ep #00259 in 006 steps, cumul_reward = 1.0\nEnd of ep #00260 in 006 steps, cumul_reward = 1.0\nEnd of ep #00261 in 009 steps, cumul_reward = 1.0\nEnd of ep #00262 in 006 steps, cumul_reward = 1.0\nEnd of ep #00263 in 006 steps, cumul_reward = 1.0\nEnd of ep #00264 in 006 steps, cumul_reward = 1.0\nEnd of ep #00265 in 006 steps, cumul_reward = 1.0\nEnd of ep #00266 in 007 steps, cumul_reward = 1.0\nEnd of ep #00267 in 009 steps, cumul_reward = 1.0\nEnd of ep #00268 in 006 steps, cumul_reward = 1.0\nEnd of ep #00269 in 006 steps, cumul_reward = 1.0\nEnd of ep #00270 in 006 steps, cumul_reward = 1.0\nEnd of ep #00271 in 006 steps, cumul_reward = 1.0\nEnd of ep #00272 in 006 steps, cumul_reward = 1.0\nEnd of ep #00273 in 010 steps, cumul_reward = 1.0\nEnd of ep #00274 in 006 steps, cumul_reward = 1.0\nEnd of ep #00275 in 006 steps, cumul_reward = 1.0\nEnd of ep #00276 in 006 steps, cumul_reward = 1.0\nEnd of ep #00277 in 006 steps, cumul_reward = 1.0\nEnd of ep #00278 in 007 steps, cumul_reward = 1.0\nEnd of ep #00279 in 008 steps, cumul_reward = 1.0\nEnd of ep #00280 in 006 steps, cumul_reward = 1.0\nEnd of ep #00281 in 006 steps, cumul_reward = 1.0\nEnd of ep #00282 in 006 steps, cumul_reward = 1.0\nEnd of ep #00283 in 007 steps, cumul_reward = 1.0\nEnd of ep #00284 in 006 steps, cumul_reward = 1.0\nEnd of ep #00285 in 006 steps, cumul_reward = 1.0\nEnd of ep #00286 in 006 steps, cumul_reward = 1.0\nEnd of ep #00287 in 006 steps, cumul_reward = 1.0\nEnd of ep #00288 in 006 steps, cumul_reward = 1.0\nEnd of ep #00289 in 007 steps, cumul_reward = 1.0\nEnd of ep #00290 in 006 steps, cumul_reward = 1.0\nEnd of ep #00291 in 009 steps, cumul_reward = 1.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEnd of ep #00292 in 006 steps, cumul_reward = 1.0\nEnd of ep #00293 in 006 steps, cumul_reward = 1.0\nEnd of ep #00294 in 006 steps, cumul_reward = 1.0\nEnd of ep #00295 in 006 steps, cumul_reward = 1.0\nEnd of ep #00296 in 006 steps, cumul_reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "class TabQLearningControlerFeedback(object):\n",
    "    def __init__(self, env_size, n_action, gamma, lr, expected_exploration_steps, margin):\n",
    "        \n",
    "        self.n_action = n_action\n",
    "        self.env_size = env_size\n",
    "        self.q_tab = np.zeros((env_size, n_action))\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.margin = margin\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.expected_exploration_steps = expected_exploration_steps\n",
    "        self.n_step_eps = 0\n",
    "        self.minimum_epsilon = 0.05\n",
    "        self.epsilon_init = 1   \n",
    "        self.current_eps = self.epsilon_init\n",
    "    def choose_action_eps_greedy(self, s):\n",
    "        \n",
    "        s = s['state']\n",
    "        \n",
    "        if np.random.random() < self.current_eps:\n",
    "            a = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            a = np.argmax(self.q_tab[s])\n",
    "            \n",
    "        self.current_eps = max(self.minimum_epsilon, self.epsilon_init * np.exp(- 2.5 * self.n_step_eps / self.expected_exploration_steps))\n",
    "        self.n_step_eps += 1\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def optimize(self, state, action, next_state, r):\n",
    "        \n",
    "        state = state['state']\n",
    "\n",
    "        gave_feedback = next_state['gave_feedback']        \n",
    "        next_state = next_state['state']\n",
    "\n",
    "        if gave_feedback:\n",
    "            bad_action = action\n",
    "            q_with_margin = [self.q_tab[state, a] - self.margin_loss(bad_action, a) for a in range(self.n_action)]\n",
    "            self.q_tab[state, action] = (1 - self.lr) * self.q_tab[state, action] + self.lr * min(q_with_margin)\n",
    "        else:\n",
    "            self.q_tab[state, action] = (1 - self.lr) * self.q_tab[state, action] + self.lr * (r + self.gamma * np.max(self.q_tab[next_state]))\n",
    "            \n",
    "    def margin_loss(self, bad_action, a):\n",
    "        if bad_action == a:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.margin\n",
    "        \n",
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "margin=0.2\n",
    "\n",
    "create_algo = lambda : TabQLearningControlerFeedback(16, 4, gamma=gamma,\n",
    "                                                     lr=0.1, expected_exploration_steps=expected_exploration_steps, margin=margin)\n",
    "\n",
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=0)\n",
    "run_res = run_expe(create_algo, game, verbose=True, max_ep=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen-Lake Deterministe avec Feedback\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Tabulaire, pas de récompenses négatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes mean 269.805, std 172.13410752956545\nNumber of failure to solve env 48\nNumber of episodes when not failing 233.0388655462185\nTotal steps to solve environment :\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "lr=0.1\n",
    "\n",
    "algo = lambda : TabQLearning(16, 4, gamma=gamma, lr=lr, expected_exploration_steps=expected_exploration_steps)\n",
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=0)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes mean 263.204, std 155.741350912338\nNumber of failure to solve env 38\nNumber of episodes when not failing 234.13929313929313\nTotal steps to solve environment :\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "lr = 0.7\n",
    "\n",
    "\n",
    "algo = lambda : TabQLearning(16, 4, gamma=gamma, lr=lr, expected_exploration_steps=expected_exploration_steps)\n",
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=0)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Tabulaire, récompenses négatives de l'environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes mean 263.376, std 156.38819208623138\nNumber of failure to solve env 37\nNumber of episodes when not failing 235.11214953271028\nTotal steps to solve environment :\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "reward_when_falling = -0.1\n",
    "\n",
    "algo = lambda : TabQLearning(16, 4, gamma=gamma, lr=0.1, expected_exploration_steps=expected_exploration_steps)\n",
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=reward_when_falling)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "lr = 0.7\n",
    "reward_when_falling = -0.1\n",
    "\n",
    "algo = lambda : TabQLearning(16, 4, gamma=gamma, lr=lr, expected_exploration_steps=expected_exploration_steps)\n",
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=reward_when_falling)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Tabulaire inclusion du Feedback, pas de récompenses négatives de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes mean 32.568\nNumber of failure to solve env 27\nNumber of episodes when not failing 7.787692307692308\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "margin= 0.2\n",
    "\n",
    "algo = lambda : TabQLearningControlerFeedback(16, 4, gamma=gamma, lr=0.1, expected_exploration_steps=expected_exploration_steps, margin=margin)\n",
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=0)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen-Lake Non-Deterministe avec Feedback\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deterministe {0: {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 4, 0.0, False)], 2: [(1.0, 1, 0.0, False)], 3: [(1.0, 0, 0.0, False)]}, 1: {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 5, 0.0, True)], 2: [(1.0, 2, 0.0, False)], 3: [(1.0, 1, 0.0, False)]}, 2: {0: [(1.0, 1, 0.0, False)], 1: [(1.0, 6, 0.0, False)], 2: [(1.0, 3, 0.0, False)], 3: [(1.0, 2, 0.0, False)]}, 3: {0: [(1.0, 2, 0.0, False)], 1: [(1.0, 7, 0.0, True)], 2: [(1.0, 3, 0.0, False)], 3: [(1.0, 3, 0.0, False)]}, 4: {0: [(1.0, 4, 0.0, False)], 1: [(1.0, 8, 0.0, False)], 2: [(1.0, 5, 0.0, True)], 3: [(1.0, 0, 0.0, False)]}, 5: {0: [(1.0, 5, 0, True)], 1: [(1.0, 5, 0, True)], 2: [(1.0, 5, 0, True)], 3: [(1.0, 5, 0, True)]}, 6: {0: [(1.0, 5, 0.0, True)], 1: [(1.0, 10, 0.0, False)], 2: [(1.0, 7, 0.0, True)], 3: [(1.0, 2, 0.0, False)]}, 7: {0: [(1.0, 7, 0, True)], 1: [(1.0, 7, 0, True)], 2: [(1.0, 7, 0, True)], 3: [(1.0, 7, 0, True)]}, 8: {0: [(1.0, 8, 0.0, False)], 1: [(1.0, 12, 0.0, True)], 2: [(1.0, 9, 0.0, False)], 3: [(1.0, 4, 0.0, False)]}, 9: {0: [(1.0, 8, 0.0, False)], 1: [(1.0, 13, 0.0, False)], 2: [(1.0, 10, 0.0, False)], 3: [(1.0, 5, 0.0, True)]}, 10: {0: [(1.0, 9, 0.0, False)], 1: [(1.0, 14, 0.0, False)], 2: [(1.0, 11, 0.0, True)], 3: [(1.0, 6, 0.0, False)]}, 11: {0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}, 12: {0: [(1.0, 12, 0, True)], 1: [(1.0, 12, 0, True)], 2: [(1.0, 12, 0, True)], 3: [(1.0, 12, 0, True)]}, 13: {0: [(1.0, 12, 0.0, True)], 1: [(1.0, 13, 0.0, False)], 2: [(1.0, 14, 0.0, False)], 3: [(1.0, 9, 0.0, False)]}, 14: {0: [(1.0, 13, 0.0, False)], 1: [(1.0, 14, 0.0, False)], 2: [(1.0, 15, 1.0, True)], 3: [(1.0, 10, 0.0, False)]}, 15: {0: [(1.0, 15, 0, True)], 1: [(1.0, 15, 0, True)], 2: [(1.0, 15, 0, True)], 3: [(1.0, 15, 0, True)]}}\nnon deterministic {0: {0: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)], 1: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 2: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)], 3: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]}, 1: {0: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True)], 1: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 2: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)], 3: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False)]}, 2: {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False)], 1: [(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False)], 2: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 1, 0.0, False)]}, 3: {0: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 1: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)], 2: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False)], 3: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2, 0.0, False)]}, 4: {0: [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)], 1: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)], 2: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)], 3: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]}, 5: {0: [(1.0, 5, 0, True)], 1: [(1.0, 5, 0, True)], 2: [(1.0, 5, 0, True)], 3: [(1.0, 5, 0, True)]}, 6: {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]}, 7: {0: [(1.0, 7, 0, True)], 1: [(1.0, 7, 0, True)], 2: [(1.0, 7, 0, True)], 3: [(1.0, 7, 0, True)]}, 8: {0: [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True)], 1: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False)], 2: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False)], 3: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]}, 9: {0: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False)], 1: [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False)], 2: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True)], 3: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 8, 0.0, False)]}, 10: {0: [(0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False)], 1: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True)], 2: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False)], 3: [(0.3333333333333333, 11, 0.0, True), (0.3333333333333333, 6, 0.0, False), (0.3333333333333333, 9, 0.0, False)]}, 11: {0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}, 12: {0: [(1.0, 12, 0, True)], 1: [(1.0, 12, 0, True)], 2: [(1.0, 12, 0, True)], 3: [(1.0, 12, 0, True)]}, 13: {0: [(0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False)], 1: [(0.3333333333333333, 12, 0.0, True), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)], 2: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False)], 3: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 9, 0.0, False), (0.3333333333333333, 12, 0.0, True)]}, 14: {0: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)], 1: [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)], 2: [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)], 3: [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]}, 15: {0: [(1.0, 15, 0, True)], 1: [(1.0, 15, 0, True)], 2: [(1.0, 15, 0, True)], 3: [(1.0, 15, 0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "game = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=0)\n",
    "print(\"deterministe\", game.P)\n",
    "\n",
    "\n",
    "game = FrozenLakeEnvStop(is_slippery=True, reward_when_falling=0)\n",
    "print(\"non deterministic\", game.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\nNone\nEnd of ep #00000 in 069 steps, cumul_reward = 1.1102230246251565e-16\nEnd of ep #00001 in 100 steps, cumul_reward = -1.4000000000000001\nEnd of ep #00002 in 022 steps, cumul_reward = 0.8\nEnd of ep #00003 in 100 steps, cumul_reward = -1.7000000000000004\nEnd of ep #00004 in 049 steps, cumul_reward = -0.19999999999999996\nEnd of ep #00005 in 088 steps, cumul_reward = -0.8000000000000005\nEnd of ep #00006 in 044 steps, cumul_reward = 0.6\nEnd of ep #00007 in 090 steps, cumul_reward = 0.20000000000000007\nEnd of ep #00008 in 030 steps, cumul_reward = 0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "margin=0.2\n",
    "\n",
    "create_algo = lambda : TabQLearningControlerFeedback(16, 4, gamma=gamma,\n",
    "                                                     lr=0.1, expected_exploration_steps=expected_exploration_steps, margin=margin)\n",
    "\n",
    "game = FrozenLakeEnvStop(is_slippery=True, reward_when_falling=-0.1)\n",
    "\n",
    "run_expe(create_algo, game, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems very easy to solve because of the controller stopping from doing stupid stuff, the explo is easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Tabulaire, pas de récompenses négatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes mean 15.509\nNumber of failure to solve env 4\nNumber of episodes when not failing 13.53807615230461\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "lr = 0.7\n",
    "\n",
    "algo = lambda : TabQLearning(16, 4, gamma=gamma, lr=lr, expected_exploration_steps=expected_exploration_steps)\n",
    "game = FrozenLakeEnvStop(is_slippery=True, reward_when_falling=0)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Tabulaire, récompenses négatives de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes mean 15.721\nNumber of failure to solve env 0\nNumber of episodes when not failing 15.721\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "lr = 0.7\n",
    "\n",
    "algo = lambda : TabQLearning(16, 4, gamma=gamma, lr=lr, expected_exploration_steps=expected_exploration_steps)\n",
    "game = FrozenLakeEnvStop(is_slippery=True, reward_when_falling=-0.1)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Tabulaire inclusion du Feedback, pas de récompenses négatives de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes mean 20.311\nNumber of failure to solve env 1\nNumber of episodes when not failing 20.311\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "expected_exploration_steps = 3000\n",
    "margin= 0.2\n",
    "lr = 0.7\n",
    "\n",
    "\n",
    "algo = lambda : TabQLearningControlerFeedback(16, 4, gamma=gamma, lr=lr, expected_exploration_steps=expected_exploration_steps, margin=margin)\n",
    "game = FrozenLakeEnvStop(is_slippery=True, reward_when_falling=0)\n",
    "\n",
    "run_multiple_expe(algo, game, n_expe=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, size_in, n_action):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_hidden = 10\n",
    "        self.n_action = n_action\n",
    "        self.input_size = size_in\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, self.n_hidden)\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.n_action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            qs = policy_net(state)\n",
    "            return qs.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #0, cumul_reward : 35.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #40, cumul_reward : 42.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #80, cumul_reward : 16.525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #120, cumul_reward : 25.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #160, cumul_reward : 139.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #200, cumul_reward : 153.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #240, cumul_reward : 197.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #280, cumul_reward : 148.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #320, cumul_reward : 215.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #360, cumul_reward : 202.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #400, cumul_reward : 186.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #440, cumul_reward : 153.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #480, cumul_reward : 153.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #520, cumul_reward : 153.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #560, cumul_reward : 120.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #600, cumul_reward : 124.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #640, cumul_reward : 154.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #680, cumul_reward : 122.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #720, cumul_reward : 129.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #760, cumul_reward : 54.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #800, cumul_reward : 171.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #840, cumul_reward : 131.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #880, cumul_reward : 253.675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #920, cumul_reward : 202.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #960, cumul_reward : 168.625\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 40\n",
    "LR = 6e-3\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "env_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = QModel(size_in=env_size, n_action=n_actions).to(device)\n",
    "target_net = QModel(size_in=env_size, n_action=n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "num_episodes = 1000\n",
    "cumul_reward = 0\n",
    "rew_list = []\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = torch.FloatTensor(env.reset()).unsqueeze(0)\n",
    "    assert isinstance(state, torch.Tensor)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        cumul_reward += reward\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        assert isinstance(next_state, torch.Tensor)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            rew_list.append(cumul_reward)\n",
    "            cumul_reward = 0\n",
    "            break\n",
    "    \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(\"End of ep #{}, cumul_reward : {}\".format(i_episode, np.mean(rew_list)))\n",
    "        rew_list = []\n",
    "\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #0, cumul_reward : 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #30, cumul_reward : -0.1233333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #60, cumul_reward : 0.043333333333333314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #90, cumul_reward : 0.019999999999999976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #120, cumul_reward : 0.13000000000000017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #150, cumul_reward : -0.08666666666666632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #180, cumul_reward : -0.1966666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #210, cumul_reward : 0.006666666666666685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #240, cumul_reward : -0.4333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #270, cumul_reward : -1.3133333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #300, cumul_reward : -1.516666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #330, cumul_reward : -0.2033333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #360, cumul_reward : 0.29000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #390, cumul_reward : 0.25333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #420, cumul_reward : -0.14666666666666664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #450, cumul_reward : 0.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #480, cumul_reward : -0.020000000000000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #510, cumul_reward : -0.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #540, cumul_reward : -0.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #570, cumul_reward : -0.43333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #600, cumul_reward : -0.24333333333333337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #630, cumul_reward : -0.2966666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #660, cumul_reward : -0.5033333333333329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #690, cumul_reward : -0.06666666666666668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #720, cumul_reward : -0.04666666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #750, cumul_reward : -0.04000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #780, cumul_reward : -0.11333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #810, cumul_reward : -0.06333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #840, cumul_reward : -0.0033333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #870, cumul_reward : -0.0033333333333333335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #900, cumul_reward : -0.04000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #930, cumul_reward : -0.07666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #960, cumul_reward : -0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of ep #990, cumul_reward : -0.04000000000000001\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "cumul_reward = 0\n",
    "rew_list = []\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 30\n",
    "LR = 3e-3\n",
    "\n",
    "env = FrozenLakeEnvStop(is_slippery=False, reward_when_falling=-0.1)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state_size = 1\n",
    "\n",
    "\n",
    "policy_net = QModel(size_in=state_size, n_action=n_actions).to(device)\n",
    "target_net = QModel(size_in=state_size, n_action=n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor([state]).unsqueeze(0)\n",
    "    assert isinstance(state, torch.Tensor)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        cumul_reward += reward\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        next_state = torch.FloatTensor([next_state]).unsqueeze(0)\n",
    "        assert isinstance(next_state, torch.Tensor)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            rew_list.append(cumul_reward)\n",
    "            cumul_reward = 0\n",
    "            break\n",
    "    \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        print(\"End of ep #{}, cumul_reward : {}\".format(i_episode, np.mean(rew_list)))\n",
    "        rew_list = []\n",
    "\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
